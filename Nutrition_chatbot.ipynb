{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d1534d53",
      "metadata": {
        "id": "d1534d53"
      },
      "outputs": [],
      "source": [
        "#step-1\n",
        "#!pip install --upgrade langchain langchain-community faiss-cpu pypdf tiktoken sentence-transformers transformers accelerate bitsandbytes\n",
        "!pip install --upgrade langchain langchain-community faiss-cpu pypdf tiktoken sentence-transformers transformers accelerate bitsandbytes pdfminer.six\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1d99a214",
      "metadata": {
        "id": "1d99a214"
      },
      "outputs": [],
      "source": [
        "#step-2\n",
        "# to upload the pdf file\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload your PDF file (e.g., 'The Basics of Nutrition.pdf').\")\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    pdf_path = list(uploaded.keys())[0]\n",
        "    print(f\"Uploaded: {pdf_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error uploading file: {e}\")\n",
        "    # Fallback for local testing if file not uploaded\n",
        "    pdf_path = 'The Basics of Nutrition.pdf' # <--- IMPORTANT: Change this if your PDF has a different name\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"Error: PDF file '{pdf_path}' not found. Please upload it or ensure it's in the correct directory.\")\n",
        "        import sys\n",
        "        sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step-3\n",
        "#Function to clean and refine text for better readability\n",
        "\n",
        "def clean_output(text):\n",
        "    import re\n",
        "\n",
        "    # Normalize newlines and tabs to single spaces first\n",
        "    text = re.sub(r'[\\n\\t]+', ' ', text)\n",
        "\n",
        "    # Aggressively remove common OCR artifacts like 'E' and 'O' that appear to join words or as noise.\n",
        "    # This specifically targets cases where 'E' or 'O' are used as separators,\n",
        "    # or when words are simply merged without a space.\n",
        "    # Example: \"FoodsE ProteinsE\" -> \"Foods Proteins\"\n",
        "    # Example: \"Ironintakesrequired\" -> \"Iron intakes required\"\n",
        "\n",
        "    # Insert space before uppercase letter if preceded by lowercase (e.g., \"WordWord\" -> \"Word Word\")\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Remove 'E' or 'O' when they are between two letters\n",
        "    text = re.sub(r'([a-zA-Z])E([a-zA-Z])', r'\\1\\2', text)\n",
        "    text = re.sub(r'([a-zA-Z])O([a-zA-Z])', r'\\1\\2', text)\n",
        "\n",
        "    # Remove isolated 'E' or 'O' characters or sequences of them (e.g., \" E \", \" O \", \" EE \")\n",
        "    text = re.sub(r'\\b[EO]\\b', ' ', text) # Standalone 'E' or 'O' as whole words\n",
        "    text = re.sub(r'(\\s*[EO]\\s*)+', ' ', text) # Sequences of 'E's or 'O's with spaces\n",
        "\n",
        "    # Remove specific header/footer remnants commonly seen in your PDF\n",
        "    text = re.sub(r'M DUL \\d+ – TH BASICS F NUTRITI N © ACF – Technical and Research Department – \\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Skerrett and Willett Page \\d+', '', text)\n",
        "    text = re.sub(r'NIH-PA Author Manuscript', '', text)\n",
        "    text = re.sub(r'E+', '', text) # Remove any remaining 'E's\n",
        "\n",
        "    # Remove long sequences of non-alphanumeric characters (like dashes, underscores, etc.)\n",
        "    text = re.sub(r\"[-=_~]{3,}\", \"\", text)\n",
        "\n",
        "    # Remove excessive repetition of the same word (e.g., \"word word word\" -> \"word\")\n",
        "    text = re.sub(r\"(\\b\\w+\\b)(?:\\s+\\1\\b)+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Collapse multiple spaces into a single space\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    # Collapse multiple newlines into a single newline\n",
        "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
        "\n",
        "    # Remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    # Ensure proper sentence formatting\n",
        "    # Attempt to capitalize the first letter of the cleaned output if it's not already\n",
        "    if text and len(text) > 0 and text[0].islower():\n",
        "        text = text[0].upper() + text[1:]\n",
        "\n",
        "    # Add a period if the sentence ends abruptly without punctuation, and it's not a list item.\n",
        "    if text and not re.search(r'[.!?]$', text) and not text.startswith('- '):\n",
        "        text += '.'\n",
        "\n",
        "    # Final pass to fix common OCR merging issues that result in repeated fragments or awkward spacing\n",
        "    text = re.sub(r'(\\w+)\\s*>\\s*(\\w+)', r'\\1 > \\2', text) # 'Word>Word' -> 'Word > Word'\n",
        "    text = re.sub(r'([a-z])(\\s+)(\\w)', r'\\1 \\3', text) # Remove extra space after a lowercase letter followed by a capital letter, if one was added by the first rule (e.g. \"Acid, minerals\" -> \"Acid, minerals\" if previous rule added space)\n",
        "    text = re.sub(r'(\\w+),(\\w+)', r'\\1, \\2', text) # Ensure space after comma if word follows immediately\n",
        "    text = re.sub(r'\\.(\\w)', r'. \\1', text) # Ensure space after period if word follows immediately\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "VjFFTlpCE3m0"
      },
      "id": "VjFFTlpCE3m0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "84951479",
      "metadata": {
        "id": "84951479"
      },
      "outputs": [],
      "source": [
        "#step-4: Load and chunk the PDF\n",
        "#Optimizing chunk size and overlap can improve retrieval accuracy.\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from pdfminer.high_level import extract_text # For alternative extraction\n",
        "\n",
        "print(f\"Loading PDF from: {pdf_path}\")\n",
        "try:\n",
        "    # Try extracting text using pdfminer.six first\n",
        "    raw_pdf_text = extract_text(pdf_path)\n",
        "\n",
        "    # Split the raw text into pages using form feed character, then clean each page\n",
        "    pages_content_list = raw_pdf_text.split('\\f')\n",
        "\n",
        "    documents = []\n",
        "    for i, page_content in enumerate(pages_content_list):\n",
        "        cleaned_page_content = clean_output(page_content) # Clean each page immediately\n",
        "        if cleaned_page_content.strip(): # Only add if content is not just empty after cleaning\n",
        "            documents.append(Document(page_content=cleaned_page_content, metadata={\"page\": i + 1, \"source\": pdf_path}))\n",
        "\n",
        "    print(f\"Extracted and initially cleaned {len(documents)} document-like sections.\")\n",
        "\n",
        "    # Now, use Langchain's splitter on these pre-cleaned documents\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40) # Keep optimized for LLM input\n",
        "    chunks = splitter.split_documents(documents) # Use the pre-cleaned documents\n",
        "\n",
        "    print(f\"Split cleaned document sections into {len(chunks)} final chunks.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or chunking PDF: {e}\")\n",
        "    chunks = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f9d4536c",
      "metadata": {
        "id": "f9d4536c"
      },
      "outputs": [],
      "source": [
        "#setp-5\n",
        "#Generate vector embeddings using Hugging Face\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "print(\"Generating vector embeddings...\")\n",
        "try:\n",
        "    # Using a good default embedding model from Hugging Face\n",
        "    embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    # Use the 'chunks' list which now contains pre-cleaned content\n",
        "    vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "    print(\"Vector store created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating embeddings or creating vector store: {e}\")\n",
        "    vectorstore = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4a3eeab7",
      "metadata": {
        "id": "4a3eeab7"
      },
      "outputs": [],
      "source": [
        "#step-6: Use a free Hugging Face model for LLM\n",
        "\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"Loading Hugging Face LLM...\")\n",
        "try:\n",
        "    # --- IMPORTANT CHANGE FOR PERFORMANCE AND STABILITY ---\n",
        "    # Reverting to 'flan-t5-base' for better performance in environments with limited resources.\n",
        "    # If you have a powerful GPU (e.g., A100), you can try 'google/flan-t5-large' again.\n",
        "    model_name = \"google/flan-t5-base\" # Using 'base' for faster execution and less memory\n",
        "    # model_name = \"google/flan-t5-large\" # Uncomment this if 'large' causes memory issues and your environment can handle it\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # Using `torch_dtype=torch.bfloat16` and `device_map=\"auto\"` for memory efficiency on GPU\n",
        "    # If you don't have a GPU or encounter errors, you might need to remove `torch_dtype` or set `device_map=\"cpu\"`\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None, # Use bfloat16 only if CUDA is available\n",
        "        device_map=\"auto\" # Automatically maps layers to available devices (GPU/CPU)\n",
        "    )\n",
        "\n",
        "    # Configure the pipeline for text generation. Max_length is important for controlling response verbosity.\n",
        "    # Tuned parameters for potentially better generation quality.\n",
        "    pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=150, # Increased max_length slightly to allow for more coherent sentences\n",
        "        temperature=0.2, # Kept low for deterministic, direct output\n",
        "        top_k=50,        # Sample from top K most likely tokens.\n",
        "        top_p=0.95,      # Nucleus sampling: sample from tokens summing up to P probability.\n",
        "        repetition_penalty=1.1, # Penalize repeating tokens, reducing repetitive output.\n",
        "        do_sample=True,   # Enable sampling (required for temperature, top_k, top_p)\n",
        "        num_return_sequences=1 # Generate only one sequence\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    print(f\"Hugging Face LLM '{model_name}' loaded successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Hugging Face LLM: {e}\")\n",
        "    llm = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "066a6454",
      "metadata": {
        "id": "066a6454"
      },
      "outputs": [],
      "source": [
        "# step-7: Set up the RAG Chain\n",
        "# Langchain's RetrievalQA chain is excellent for combining retrieval and generation.\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "print(\"Setting up RAG chain...\")\n",
        "if vectorstore and llm:\n",
        "    # `search_type=\"similarity\"` is good for semantic search.\n",
        "    # --- IMPORTANT CHANGE: Set 'k' to 2 for a slightly broader context, but rely on cleaning and prompt for conciseness ---\n",
        "    retriever = vectorstore.as_retriever(search_type=\"similarity\", k=2) # Increased k to 2 for a slightly broader context\n",
        "\n",
        "    # Define the custom prompt template for better accuracy and control.\n",
        "    # This instructs the LLM to use *only* the provided context and be highly concise.\n",
        "    qa_template = \"\"\"You are a helpful assistant specialized in nutrition, based on the provided text.\n",
        "    Use ONLY the following pieces of information (context) to answer the question at the end.\n",
        "    If the context does not contain enough information to answer the question, state clearly and concisely: \"I don't have enough information from the provided document to answer that question.\"\n",
        "    Do NOT try to make up an answer or use external knowledge.\n",
        "    Be extremely concise and direct. Provide the exact answer from the context if possible, otherwise summarize it in one to two sentences.\n",
        "    Ensure your answer is grammatically correct and flows naturally.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "    QA_PROMPT = PromptTemplate(template=qa_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        # --- IMPORTANT CHANGE: Do not return source documents in the final output ---\n",
        "        return_source_documents=False, # Set to False to not show source details in the final output\n",
        "        chain_type=\"stuff\", # 'stuff' concatenates all retrieved documents into the prompt\n",
        "        chain_type_kwargs={\"prompt\": QA_PROMPT} # Pass the custom prompt\n",
        "    )\n",
        "    print(\"RAG chain setup complete.\")\n",
        "else:\n",
        "    print(\"RAG chain could not be set up due to previous errors. Please check previous cell outputs.\")\n",
        "    rag_chain = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 7: Ask questions\n",
        "\n",
        "print(\"\\n--- Basics of Nutrition Chatbot (Hugging Face) ---\")\n",
        "print(\"Ask me anything about nutrition from the PDF. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYour Question: \")\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if not rag_chain:\n",
        "        print(\"Chatbot is not initialized due to previous errors. Cannot process query.\")\n",
        "        continue\n",
        "\n",
        "    print(\"Thinking...\")\n",
        "    try:\n",
        "        # Use .invoke() for newer Langchain versions and pass a dictionary as input\n",
        "        result = rag_chain.invoke({\"query\": query})\n",
        "        answer = result['result']\n",
        "        # source_documents = result['source_documents'] # No longer returned if return_source_documents=False\n",
        "\n",
        "        cleaned_answer = clean_output(answer)\n",
        "        print(f\"Answer: {cleaned_answer}\")\n",
        "\n",
        "        # --- IMPORTANT CHANGE: Removed the source display logic ---\n",
        "        # if source_documents:\n",
        "        #     print(\"\\n--- Sources Used ---\")\n",
        "        #     for i, doc in enumerate(source_documents):\n",
        "        #         print(f\"Source {i+1} (Page {doc.metadata.get('page', 'N/A')}):\")\n",
        "        #         cleaned_snippet = clean_output(doc.page_content)\n",
        "        #         print(f\"  Snippet: {cleaned_snippet[:200]}...\")\n",
        "        #         print(\"-\" * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"Please try re-running the notebook or check for specific error messages above.\")\n"
      ],
      "metadata": {
        "id": "NBCiJhGcBsMx"
      },
      "id": "NBCiJhGcBsMx",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgF0Arl6BsV9"
      },
      "id": "qgF0Arl6BsV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJg_FcWeBsep"
      },
      "id": "iJg_FcWeBsep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTATBdsxBsf4"
      },
      "id": "vTATBdsxBsf4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}